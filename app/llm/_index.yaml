version: "1.0"
namespace: app.llm

entries:
  ###########################
  #   Dependency entries    #
  ###########################

  - name: __dependency.wippy.llm
    kind: "ns.dependency"
    meta:
      description: "LLM component"
    component: "wippy/llm"
    version: ">=v0.0.7"
    parameters:
      - name: "host"
        value: "app:processes"

  - name: application_host
    kind: ns.definition
    targets:
      - entry: __dependency.wippy.llm
        path: .parameters[] | select(.name == "host") | .value

  ###########################
  #   Function entries      #
  ###########################

  - name: openai
    kind: library.lua
    meta:
      comment: "OpenAI LLM integration for chat sessions"
    source: file://openai.lua
    modules: [ "json" ]
    imports:
      llm: wippy.llm:llm
      prompt: wippy.llm:prompt

  - name: openai.llm_query
    kind: function.lua
    meta:
      comment: "LLM query function for chat integration"
    source: file://openai.lua
    method: llm_query
    modules: [ "json" ]
    imports:
      llm: wippy.llm:llm
      prompt: wippy.llm:prompt
